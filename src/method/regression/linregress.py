import numpy as np
from enum import Enum, auto
from multiprocessing import Pool,cpu_count

class SAPARATE_DELETE(Enum):
    RANSAC=auto()
    LMedS=auto()
    Zscore=auto()

class LinRegressor:
    def __init__(self,
                 x:np.ndarray, y:np.ndarray,
                 weights:np.ndarray=None,
                 del_saparated_point:bool=False,
                 optim_method=SAPARATE_DELETE.RANSAC,
                 threshold:float=3,
                 max_iter:int=100,
                 transform_xy:callable=None,
                 *args,**kwargs):
        """One-independent-variable Linear Regressor

        Args:
            x (np.ndarray): values of independent variable, should be 1Dcould be 1D or 2D (dim 0: data).
            y (np.ndarray): values of dependent variable, could be 1D or 2D \
                (dim 0: data for different x; dim 1: parallel values for certain x).
            weights (np.ndarray): weights of each point, could be None if you do not need weighted regression.
            del_saparated_point (bool, optional): choose whether to delete separated points from dataset or not.\
                Defaults to False.
            optim_method (_type_, optional): method to delete separated points, \
                could be selected from Enum object SAPARATE_DELETE. Defaults to SAPARATE_DELETE.RANSAC.
            threshold (float, optional): threshold for deleting separated points. \
                
                In **RANSAC**, point i will be recognized as inliers if |residue_i|<threshold*SD(residue);\
                    
                In **LMedS**, point i will be recognized as inliers if |residue_i|<sqrt(threshold*median(residue^2));\
                    
                In **Zscore**, point i will be deleted if |Z|>threshold (where Z=(residue-mu)/sigma)) \
                Defaults to 3.
            max_iter (int, optional): RANSAC and LMedS need to select some points from dataset to train many\
                small linear models parallelly, then select a best one. max_iter tells the function\
                how many small models should be trained. Defaults to 100.
            transform_xy (callable, optional): In original data, x and y may not be fit for a linear\
                model. This is an interface to input a function like this:
                    ```
                    def Arrhenius(T:float, k:float):
                        return 1/T, np.log(k)
                    ```
                module RegressUtils.py provides many functions like this.
        """
        if len(x)!=len(y):
            if len(y.shape)==2:
                if len(x)==y.shape[1]:
                    y=y.T.copy()
                else:
                    raise ValueError("x and y are not in the same length.")
        if not transform_xy is None:
            self.transform=transform_xy
            self.x,self.y=transform_xy(x,y)
        else:
            self.x, self.y=x, y
        self.length=len(x)
        self.mean_x=np.mean(x)
        self.weights=weights
        if len(self.y.shape)==1:
            self.mean_y=np.mean(y)
            self.multi_y=False
        elif len(self.y.shape)==2:
            self.std_y=np.nanstd(self.y, axis=1)
            self.y=np.nanmean(self.y, axis=1)
            self.mean_y=np.mean(self.y)
            self.multi_y=True
        else:
            raise ValueError("Array y can only be 1D or 2D.")
        
        self.del_saparated_point=del_saparated_point
        if del_saparated_point:
            self.threshold=threshold
            self.optim_method=optim_method
            self.max_iter=max_iter
            
    def fit(self,use_weights=True,num_samples=3,epsilon:float=1e-2):
        """fit a linear model.

        Args:
            use_weights (bool, optional): Choose whether to use the weights if there is a weights input\
                or weights are generated by 1/SD if there are parallel experiment at each x. Defaults to True.
            num_samples (int, optional): For RANSAC and LMedS, how many points are selected to construct\
                each small model. Defaults to 2.
            epsilon (float, optional): Protection term in generating weights when input y is 2D: 1/(SD+epsilon). Defaults to 1e-2.

        Returns:
            slope: Slope of regression curve.
            intercept: Intercept of regression curve.
            pcov: Parameter covarience matrix. Elements on the diagonal is varience of each parameter.
            Rsq: correlation coefficient.
            Rsq_adj: adjusted correlation coefficient.
        """
        if not self.del_saparated_point:
            if (not self.multi_y) and (self.weights is None):
                self.slope,self.intercept=_simpleRegressor(self.x,self.y,
                                                           self.length,self.mean_x,self.mean_y)
                weights=None
            else:
                if self.weights is None:
                    self.weights=1/(self.std_y+epsilon)
                weights=self.weights
                if use_weights:
                    self.slope,self.intercept=_weightedRegressor(self.x,self.y,weights)
                else:
                    self.slope,self.intercept=_simpleRegressor(self.x,self.y,self.length,
                                                               self.mean_x,self.mean_y)
            self.pcov=_compute_linpcov(self.x,self.y,self.slope,self.intercept,weights)
            self.Rsq,self.Rsq_adj=_compute_Rsq(self.x,self.y,self.mean_y,self.slope,self.intercept)
        else:
            match self.optim_method:
                case SAPARATE_DELETE.RANSAC:
                    with Pool(processes=min(cpu_count(),8)) as pool:
                        results=pool.map(_ransac_once,[(self.x,self.y,
                                                self.threshold,num_samples)]*self.max_iter)
                    best_id=np.argmax(np.array([results[i]["num_inliers"] for i in range(len(results))]))
                    self.inlier_id=results[best_id]["inliers"]
                    self.slope,self.intercept,self.pcov, self.Rsq,self.Rsq_adj=\
                    _fit_final_model(self.x[self.inlier_id],self.y[self.inlier_id])
                case SAPARATE_DELETE.LMedS:
                    with Pool(processes=min(cpu_count(),8)) as pool:
                        results=pool.map(_lmeds_once,[(self.x,self.y,num_samples)]*self.max_iter)
                    best_id=np.argmin(np.array([results[i]["resmed"] for i in range(len(results))]))
                    self.inlier_id=_get_inliers(self.x,self.y,results[best_id]["slope"],
                                           results[best_id]["intercept"],
                                           np.sqrt(self.threshold*results[best_id]["resmed"]))
                    self.slope,self.intercept,self.pcov, self.Rsq,self.Rsq_adj=\
                    _fit_final_model(self.x[self.inlier_id],self.y[self.inlier_id])
                case SAPARATE_DELETE.Zscore:
                    slope_,intercept_=_simpleRegressor(self.x,self.y,self.length,
                                                       self.mean_x,self.mean_y)
                    self.inlier_id=_get_inliers(self.x,self.y,slope_,intercept_,
                                self.threshold*np.std(self.y-slope_*self.x-intercept_))
                    self.slope,self.intercept,self.pcov, self.Rsq,self.Rsq_adj=\
                    _fit_final_model(self.x[self.inlier_id],self.y[self.inlier_id])
        return self.slope, self.intercept, self.pcov, self.Rsq, self.Rsq_adj
    
    def __call__(self, x, *args, **kwds):
        try:
            return self.slope*x+self.intercept
        except:
            raise RuntimeError("Invalid input or undefined parameters (use LinRegressor.fit() first)")
        
    def plot(self,ax,decimals=4,*args,**kwargs):
        """draw the regression curve.

        Args:
            ax : matplotlib canvas object
            decimals (int, optional): For printing the expression of the curve,\
                how many decimals will be retained. Defaults to 4.

        Returns:
            line: regression curve, a matplotlib Line2D object
        """
        sign="+" if self.intercept>0 else "-"
        expr=rf"$y={np.round(self.slope,decimals)}x{sign}{np.round(abs(self.intercept),decimals)}$"
        legend=kwargs.pop("legend",expr)
        line, =ax.plot([min(self.x),max(self.x)],[self(min(self.x)),self(max(self.x))],label=legend,*args,**kwargs)
        ax.figure.canvas.draw()
        ax.legend()
        return line
    
    def scatter(self,ax,*args,**kwargs):
        if self.del_saparated_point:
            dots_inlier=ax.scatter(self.x[self.inlier_id],self.y[self.inlier_id],color="red")
            dots_=[item for item in range(self.length) if item not in self.inlier_id]
            dots_others=ax.scatter(self.x[dots_],self.y[dots_],*args,**kwargs)
        else:
            dots_inlier=ax.scatter(self.x,self.y,*args,**kwargs)
            dots_others=None
        return dots_inlier,dots_others
    
    def errorbar(self,ax,*args,**kwargs):
        if self.multi_y:
            errbar=ax.errorbar(self.x, self.y, yerr=self.std_y,fmt='o',*args,**kwargs)
        return errbar
                
def _fit_final_model(x,y):
    slope,intercept=_simpleRegressor(x,y,len(x),x.mean(),y.mean())
    pcov=_compute_linpcov(x,y,slope,intercept)
    Rsq,Rsq_adj=_compute_Rsq(x,y,y.mean(),slope,intercept)
    return slope,intercept,pcov,Rsq,Rsq_adj
                
def _ransac_once(args):
    x,y,threshold,num_samples=args
    samples=np.random.choice(list(range(len(x))),num_samples)
    slope_,intercept_=_simpleRegressor(x[samples],y[samples],
                                    len(samples),x[samples].mean(),y[samples].mean())

    thr=threshold*np.std(y-slope_*x-intercept_)
    inliers_=_get_inliers(x,y,slope_,intercept_,thr)
    return {
        "slope":slope_,
        "intercept":intercept_,
        "num_inliers":len(inliers_),
        "inliers":inliers_
    }
    
def _lmeds_once(args):
    x,y,num_samples=args
    samples=np.random.choice(list(range(len(x))),num_samples)
    slope_,intercept_=_simpleRegressor(x[samples],y[samples],
                                    len(samples),x[samples].mean(),y[samples].mean())
    residue_med=np.median((y-slope_*x-intercept_)**2)
    return {
        "slope":slope_,
        "intercept":intercept_,
        "resmed":residue_med
    }
                
def _simpleRegressor(x,y,length,mean_x,mean_y):
    slope=(np.sum(x*y)/length-mean_x*mean_y)/\
                (np.sum(x**2)/length-mean_x**2)
    intercept=mean_y-slope*mean_x
    return slope, intercept
    
def _weightedRegressor(x,y,weights):
    A_=np.array([
                [np.sum(weights*x**2), np.sum(weights*x)],
                [np.sum(weights*x),    np.sum(weights)  ]
                ])
    b_=np.array([[np.sum(weights*x*y)],
                [np.sum(weights*y)]])
    slope,intercept=np.linalg.solve(A_,b_).reshape(2,)
    return slope,intercept

def _get_inliers(x,y,a,b,tol):
    return np.where(np.abs(y-a*x-b)<tol)[0]

def _compute_linpcov(x,y,a,b,weights=None):
    sigma2=np.sum((y-a*x-b)**2)/(len(x)-2)
    if weights is None:
        weights=np.eye(len(x))
    else:
        weights=np.diag(weights)
    J=np.concat((x.reshape(len(x),1),np.ones((len(x),1),dtype=float)),axis=1)
    return sigma2*np.linalg.inv(J.T.dot(weights).dot(J))

def _compute_Rsq(x,y,mean_y,a,b):
    SSres=np.sum((y-a*x-b)**2)
    SStot=np.sum((y-mean_y)**2)
    Rsq=1-SSres/SStot
    Rsq_adj=1-((1-Rsq)*(len(x)-1)/(len(x)-2))
    return Rsq,Rsq_adj